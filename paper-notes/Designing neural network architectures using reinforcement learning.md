这篇论文按照arXiv发布时间来看应该是同一时期的工作，采取了与NAS非常不同的架构。在强化学习算法上，前者用的是policy-bases算法REINFORCE，
而后者是传统的Q-learning方法，注意不是DQN，根据论文来看作者用的Q-Learning是没有包含FA的，也就是直接查表，因此感觉没有NAS的架构灵活。

这篇文章有很多的细节部分，没有细读，在这里只是总结一下一些大的框架。首先是状态空间的定义：

![](http://o7ie0tcjk.bkt.clouddn.com/ic34jwu9voavqy7l.jpg)

每个时间步的状态都是上表中四种类型之一，然后允许agent在任意时间步选择终止步从而终止网络结构的搜索。这里可以看出，这篇论文和NAS有一个逻辑定义
上的区别，NAS把这篇论文中的state定义为action，因此本文的动作空间就没有NAS中定义的自然，本文的动作空间是根据当前所处的状态不同而不同的，
为了使得最后搜索出的架构合理且加速整个训练流程，作者对action的选取增加了很多的限制，比如当当前状态处于第i层时，只能选择层数是i+1的网络类型。

下面是整个算法的伪代码：

![](http://o7ie0tcjk.bkt.clouddn.com/c3htk16c49w93ekk.jpg)
