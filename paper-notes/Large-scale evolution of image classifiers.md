论文采用遗传算法代替强化学习进行网络结构的搜索。

本文采用的遗传算法较为简单，只包含选择和变异部分，交叉操作没有采用，原因是因为本文采用的编码方式不是类似二进制编码，而是直接用有向无环图
来表示每一个个体，也就是网络结构。其中图的顶点代表tensot或者activation（batch normalization or ReLU or Linear），边代表卷积操作
或者identity操作（参照resnet，最近一篇论文表示Resnet结构同样拥有general function approximator的能力，同时比fully connected网络
理论假设更少）。同时learning rate信息也保存在个体中，这一点倒是挺少见，大部分论文都只做网络结构的搜索。

用作编码的有向无环图允许一个顶点有大于1的入度和出度。大于1的入度会带来一个问题，也就是论文假定activation（顶点）的输入必须是相同size的，
本文的做法是从所有入边中选取一条作为primary edge，这条边不能是skip connection。然后其他边必须通过reshape、channel的truncation或者oadding
去符合primary edge的维度。适应度指的是模型经过训练后在验证集上的精度。

论文采用的遗传算法具体流程如下：
1. 从一个个体开始
2. 变异，并将变异的后代加入群体中
3. 随机选择两个个体，杀掉（在总个体数不少于lower bound的情况下）适应度更低的
4. 跳转到第二步，重复这个过程直到满足终止条件

变异操作包括以下几种
- 改变学习率
- 不变
- 重置weight
- 增加卷积层
- 去除卷积层
- 改变stride （$2^{n}$）
- 改变channel
- 改变filter size （奇数）
- 加入identity连接
- 加入skip connection
- 去除skip connection

以上所有操作需要进行随机采样的都是从均匀分布进行采样，详细描述如下：

![](http://o7ie0tcjk.bkt.clouddn.com/5y8tf.jpg)

为了提高训练效率，论文采用了并行的方式进行算法迭代。本文采用的方法比较有意思，除了运行算法的worker之外（设定群体个数
的1/4，论文采用了250个worker），还有一个共享目录存放群体，这些群体不是存成文件，而是存成文件夹，然后把模型序列化之后
存到这个文件夹中。然后通过给文件夹重命名来表示当前这个个体的状态，例如训练中、已经死亡、活跃等等。

在算法开始阶段，每个worker运行的模型都只有一层隐藏层，学习率设定为0.1且不包含卷积层。论文表示这样可以尽可能少的去利用
先验信息（若变异操作设计得更加底层不知道效果如何），让算法更加充分的挖掘搜索空间。还有一点要注意的是，论文为了减少训练
难度，大部分变异操作除了变异的部分之外权重都全部保留。

论文在附录中贴出了详细的伪代码以及实现细节，最终搜索出的模型架构如下：

![](http://o7ie0tcjk.bkt.clouddn.com/z4ooa.jpg)

可以看出有一些很神奇的结构，比如多个卷积不加激活函数（完全可以用一个卷积表示），还有一个卷积后面连续跟着好几个非线性变换。
