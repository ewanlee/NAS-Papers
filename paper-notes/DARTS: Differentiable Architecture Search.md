这篇论文没有采用强化学习或者遗传算法等进行网络架构的搜索，但是效果很好，而且速度非常快。在我看来也是对NASNet的一种改进，同时结合了ENAS的思想，即在一个完全模型（有向无环图）
中去搜索子图作为网络架构。但很大的区别是，算法的整个迭代过程中是将每条边都进行考虑，而不是只是从其中用某种策略进行采样。同时考虑的方法是对于可选
操作（例如对于卷积网络来说，包括卷积，池化等操作），采用softmax的方法给每种操作产生的结果做一个加权平均，这样使得离散的网络搜索过程变成了一个
可微的优化问题。

形式化来说，每种操作都赋予一个选取概率alpha，根据alpha值进行softmax操作，其实就是一个多分类问题。这样网络结构的搜索其实就转化为求出一组alpha值，
这一组alpha值就可以看成是网络结构的一种编码。由于这alpha值是连续的，因此这就是一个可微的优化问题，loss就是根据验证集的精度得到。最后alpha训练出来
之后，每个block选alpha值最大的操作作为最终的网络结构。

因此网络结构搜索问题可以转化为以下优化问题：

![](http://o7ie0tcjk.bkt.clouddn.com/z4aqebfzhd7c4tve.jpg)

直接求解以上问题是不现实的，因为alpha每一次变化都要重新求解(4)式，作者提出了一种迭代求解的思路，这种思路很简单，但是其中有一个很重要的点就是
在固定w求解alpha的时候，w要往前再走一个梯度下降步（说实话我不是很理解，论文也没有给出这样做对收敛性有什么好处之类的证明，只不过有些论文在解
以上优化问题的时候是这么做的）。

算法流程如下：

![](http://o7ie0tcjk.bkt.clouddn.com/5dr3y7294mwai0s2.jpg)
